<!doctype html>
<html lang="en">
    <head>
        <title>Beyond Browsing: API-Based Web Agents</title>
        <link rel="icon" type="image/x-icon" href="/static/img/icons/web.png">

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Open Graph -->
        <meta property="og:url" content="https://github.com/yueqis/API-Based-Agent" />
        <meta property="og:image" content="https://pangea.github.io/static/img/preview.png" />
        <meta property="og:title" content="Beyond Browsing: API-Based Web Agents" />
        <meta property="og:description" content="Beyond Browsing: API-Based Web Agents" />
        
        <!-- Twitter -->
        <meta name="twitter:url" content="https://github.com/yueqis/API-Based-Agent" />
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:image" content="https://github.com/yueqis/API-Based-Agent/static/img/preview.png" />
        <meta name="twitter:title" content="Beyond Browsing: API-Based Web Agents" />
        <meta name="twitter:description" content="This project explores a novel approach to web agents by enabling them to use APIs in addition to traditional web-browsing techniques. By leveraging API calls, agents can perform tasks more efficiently and accurately, especially on websites with comprehensive API support." />
       
        <script src="./static/js/distill_template.v2.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

        <script defer="" src="./static/js/hider.js"></script>
        <script src="./static/js/image_interact.js"></script>
        <script src="./static/js/switch_videos.js"></script>

        <link rel="stylesheet" href="./static/css/style.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>


        <!-- medium zoom https://github.com/francoischalifour/medium-zoom -->
        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>  <!-- jquery -->
        <script defer src="./static/js/medium-zoom.min.js"></script>
        <script defer src="./static/js/zoom.js"></script>
    </head>
    <body>
        <div class="header-wrapper">
            <div class="header-container" id="header-container">
                <div class="header-content">
                    <h1 style="margin-top: 0px"><i> </i></h1>
                    <h1><i>Beyond Browsing:</i></h1>
                    <h1><i>API-Based Web Agents</i></h1>
                        <p>
                            This project explores a novel approach to web agents by enabling them to use <strong>APIs</strong> in addition to traditional web-browsing techniques. 
                            By leveraging API calls, agents can perform tasks more efficiently and accurately, especially on websites with comprehensive API support.
                        </p>
                    
                        <div class="icon-container">
                            <div class="icon-item">
                                <img src="./static/img/icons/api.png" alt="api Icon">
                                <div><strong>API-Based Agent</strong>: The API-based agent leverages application programming interfaces (APIs) to interact directly with web services, bypassing traditional web-browsing actions like simulated clicks.</div>
                            </div>
                            <div class="icon-item">
                                <img src="./static/img/icons/hybrid.png" alt="hybrid Icon">
                                <div><strong>Hybrid Agent</strong>: a agent that combines the power of API-Based Agent and traditional Web-Based Agent, capable of interleaving API calls and Web Browsing.</div>
                            </div>
                            <div class="icon-item">
                                <img src="./static/img/icons/task.png" alt="Benchmarking Icon">
                                <div><strong>Real-World Web Task Evaluation and Analysis</strong>: On WebArena, a real-world web task benchmark, our hybrid agent achieve sota performance among task-agnostic models.</div>
                            </div>
                        </div>

                    <div class="button-container">
                        <!-- replace arxiv -->
                        <a href="https://arxiv.org/abs/" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="ai ai-arxiv"></i>
                            </span>
                            arXiv
                        </a>
                        <!-- replace pdf -->
                        <a href="https://arxiv.org/pdf/" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="fas fa-file-pdf"></i>
                            </span>
                            <span>pdf</span>
                        </a>
                        <!-- replace image -->
                        <a href="https://github.com/yueqis/API-Based-Agent" class="button" target="_blank">
                            <span class="icon is-small">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>Code</span>
                        </a>
                        <!-- <br> -->                      
                    </div>
                </div>
                <div class="header-image">
                    <img draggable="false" src="static/img/icons/agent.png" alt="Teaser Image" class="teaser-image">
                </div>
            </div>
        </div>
    <d-article>
        <div class="byline">
            <div class="byline-container">
                <p>
                    <a href="https://yueqis.github.io/" class="author-link" target="_blank">Yueqi Song</a> &emsp;
                    <a href="https://frankxfz.me/" class="author-link" target="_blank">Frank Xu</a> &emsp;
                    <a href="https://shuyanzhou.github.io/" class="author-link" target="_blank">Shuyan Zhou</a> &emsp;
                    <a href="https://www.phontron.com/" class="author-link" target="_blank">Graham Neubig</a>
                    <p></p>
                    <a href="https://www.cs.cmu.edu/" class="affiliation-link" id="affiliation" target="_blank">Carnegie Mellon University</a>
                </p>
            </div>
        </div>
        
        <p class="text abstract">

            Web browsers are a portal to the internet, where much of human activity is undertaken. 
            Thus, there has been significant research work in AI agents that interact with the internet through web browsing.
            However, there is also another interface designed specifically for machine interaction with online content: application programming interfaces (APIs).
            In this paper we ask -- <strong>what if we were to take tasks traditionally tackled by browsing agents, and give AI agents access to APIs?</strong>
            To do so, we propose two varieties of agents: (1) an API-calling agent that attempts to perform online tasks through APIs only, similar to traditional coding agents, and (2) a hybrid agent that can interact with online data through both web browsing and APIs.
            In experiments on WebArena <d-cite key="zhou2023webarena"></d-cite>, a widely-used and realistic benchmark for web navigation tasks, we find that API-based agents outperform web browsing agents, as depicted in <a href="#fig:main">Figure 1</a>.
            Hybrid agents outperform both others nearly uniformly across tasks, resulting in a more than 20.0% absolute improvement over web browsing alone, achieving a success rate of 35.8%.
            These results strongly suggest that when APIs are available, they present an attractive alternative to relying on web browsing alone.
        </p>

        <figure id="fig:main">
            <img data-zoomable="" draggable="false" src="static/img/main.png" alt="main">
            <figcaption>
                <strong>Figure 1:</strong> A comparison of three types of agents. 
                The Browsing Agent performs tasks through web browsing only, utilizing the accessibility tree to interact with web pages, achieving an average performance of 14.8% on WebArena. 
                The API-Based Agent performs tasks by making API calls and generating code without relying on web browsing, achieving an average accuracy of 29.2%. 
                The Hybrid Agent combines both methods, dynamically switching between web browsing and API calling, depending on the task. 
                This allows the execution of either API calls or web browsing actions, or both in combination, improving performance by more than 5 percentage points compared to the API-Based Agent.
            </figcaption>
        </figure>

        <p class="text abstract">
            This project is structured around three key aspects:
            <ol class="text">
                <li><strong><a href="#api">&sect;API-Based Agent</a></strong>: The API-based agent is designed to interact directly with web services using structured API calls, bypassing traditional web-browsing methods like simulated clicks and form inputs. By leveraging predefined endpoints, the agent can efficiently retrieve and manipulate data, reducing the number of steps required to complete tasks. This approach not only improves task accuracy but also enhances efficiency, especially on websites with comprehensive API support.</li>
                <li><strong><a href="#hybrid">&sect;Hybrid Agent</a></strong>: The hybrid agent combines the strengths of API-based interactions and traditional web browsing by dynamically switching between the two methods based on the task requirements. This flexibility allows the agent to leverage API calls when they are available and efficient, while seamlessly resorting to web browsing actions for tasks that lack adequate API support. As a result, the hybrid agent is capable of handling a wider range of tasks with improved accuracy and efficiency compared to API-Based and traditional web agents. </li>
                <li><strong><a href="#benchmarking">&sect;Real-World Task Evaluation and Analysis</a></strong>: Are compare the agent on the WebArena benchmark. We are the first to perform a comparison of API-based agents, browsing-only agents, and hybrid agents. The results demonstrated that API-based agents outperformed browsing-only agents on websites with comprehensive API support, while the hybrid agent achieved the highest overall accuracy by dynamically switching between APIs and web browsing. Our analysis shows that the hybrid approach not only improves task efficiency but also provides greater flexibility and robustness in handling diverse and complex web interactions.</li>
            </ol>
        </p>

        <div class="icon-row">
            <a href="#browsing" class="icon-link">
                <img src="static/img/icons/web.png" alt="browsing Logo" class="icon">
                Browsing Tasks
            </a>
            <a href="#api" class="icon-link">
                <img src="static/img/icons/api.png" alt="api Logo" class="icon">
                API-Based Agent
            </a>
            <a href="#hybrid" class="icon-link">
                <img src="static/img/icons/hybrid.png" alt="hybrid Logo" class="icon">
                Hybrid Agent
            </a>
            <a href="#experiment" class="icon-link">
                <img src="static/img/icons/task.png" alt="task Logo" class="icon">
                Experiments
            </a>
            <a href="#results" class="icon-link">
                <img src="static/img/icons/result.png" alt="result Logo" class="icon">
                Results and Analysis
            </a>
        </div>

        <p class="click-hint" style="width: 85%;">
            <img src="static/img/icons/click.gif" style="width: 1.5rem">
            <strong>Click to jump to each section.</strong>
        </p>
        
        <hr>

        <div id='browsing' class="browsing-block">
            <h1 class="text">Background: Web Browsing</h1>
            <h3 class="text">The Web Browsing Task</h3>
            <p class="text">
                we focus on WebArena tasks, which simulate real-world scenarios to evaluate an agent's ability to complete diverse web-based activities.
                Tasks in WebArena include interacting with platforms like Gitlab (to manage projects and repositories), Reddit (to browse and post content), e-commerce websites (for shopping), and mapping services (for trip planning).
            </p>

            <figure id="fig:example">
                <img data-zoomable="" draggable="false" src="static/img/example.png" alt="example" style="width: 80%" class="center">
                <figcaption>
                    <strong>Figure 2:</strong> The API-based agent can often solve problems in many fewer function calls than traditional browsing agents. 
                    In this task, web browsing failed to solve the intent "find the number of commits the user `SaptakS` made to the repo `a11yproject`" after 15 steps, while our API-based agent successfully completed the task with only three lines of code.
                </figcaption>
            </figure>      


            <h3 class="text">Web Browsing Agent</h3>
            <p class="text">
                A baseline web browsing agent leverages the accessibility tree of web pages, which organizes interactive elements like buttons, input fields, and links in a hierarchical structure. 
                This structure makes it easier for agents to navigate the web by simulating human-like browsing behaviors such as clicking, filling out forms, and moving between pages. 
                The agent maintains a comprehensive history of all its previous actions, allowing it to inform its future decisions based on past interactions. 
                However, due to the complexity of some web elements and their dynamic nature, the browsing agent struggles with tasks requiring numerous or intricate interactions. 
                For example, in the task in <a href="#fig:example">Figure 2</a>, the agent needs to determine the number of commits made by a specific user to a project. 
                A traditional browsing-based approach involves logging in, navigating to the correct project, and attempting to scroll through and find the user's commits, and thus the task becomes too complex and fails after the agent's 15-step limit.
            </p>
  
        </div>

        <div id="api" class="api-block">
            <h1 class="text">From Web Browsing to API Calling</h1>
            <p class="text">
                In contrast to web browsing, API calling offer a direct interface for machines to communicate with web services, reducing operational complexity.
            </p>

            <figure id="fig:api">
                <img data-zoomable="" draggable="false" src="static/img/api.png" alt="example" style="width: 80%" class="center">
                <figcaption>
                    <strong>Figure 3:</strong> The API-based agent can often solve problems in many fewer function calls than traditional browsing agents. 
                    In this task, web browsing failed to solve the intent "find the number of commits the user `SaptakS` made to the repo `a11yproject`" after 15 steps, while our API-based agent successfully completed the task with only three lines of code.
                </figcaption>
            </figure> 

            <h3 class="text">APIs and API Documentation</h3>

            <p class="text">
                Websites with API support offer pre-defined endpoints for efficient task execution using standardized protocols like REST. 
                These APIs enable interactions via <code>HTTP</code> requests (e.g., <code>GET</code>, <code>POST</code>, <code>PUT</code>) and return structured data such as JSON objects. 
                API documentation is typically provided in formats like README, OpenAPI YAML, or plain text, offering guidance on using the APIs. 
                For instance, <a href="#fig:api">Figure 3</a> shows the official README documentation of a Gitlab API <code>GET /api/{id}/commits</code>. It documents the functionality, input arguments, and output types of the API. 
                For example, one could use the Python <code>requests</code> library, by calling <code>requests.get("gitlab.com/api/a11yproject/commits")</code>, to retrieve all commits of the repository <code>a11yproject</code>. 
                This would return a JSON list containing all the commits to this repo, as shown in <a href="#fig:api">Figure 3</a>.
            </p>

            <h3 class="text">Transitioning from Web Browsing to API Calling</h3>
            <p class="text">
                Approaches utilizing APIs is avoid complexity in web browsing, and can minimize the steps involved in complex workflows.
                For example, as discussed in <a href="#fig:example">Figure 2</a>, to accomplish the task of finding the number of commits by <code>SaptakS</code> in <code>a11yproject</code>, a basic method for implementing API calling, such as CodeAct <d-cite key="wang2024executable"></d-cite>, could directly call the repository's commit history via a simple API request <code>GET /api/a11yproject/commits</code> and then filter the results to find the commits made by <code>SaptakS</code>. 
                The task could completed in just three lines of Python code, showcasing the clear efficiency and precision that methods based on API calling could offer compared to web browsing. 
            </p>
            <h3 class="text">Obtaining APIs for Agent Use</h3>
            <p class="text">
                The process of acquiring APIs typically involves looking up official API documentation on a website.
                To maintain a structured and readable format of API documentations, we could utilize either OpenAPI YAML-style and README-style documentations. 
                Some websites might offer official YAML oor README documentations of APIs, and in such cases, in the prompt we provide API documentations sourced directly from the public API documentation provided for the website. 
                In other cases, where no official YAML or README documentations are provided, we could leverage LLMs like <a href="https://openai.com/index/hello-gpt-4o/" target="_blank">GPT-4o</a> to generate these YAML or README files. 
                By prompting GPT-4o with the relevant implementation details of the APIs (for example, the implementation of the APIs), we could generate comprehensive documentation, including input parameters, expected outputs, and example API calls, as depicted in <a href="#fig:api">Figure 3</a>.
            </p>
            <p class="text">
                <strong>One-Stage Documentation for Small API Sets:</strong>
                For websites with a smaller number of API endpoints, we directly incorporate the full documentation into the prompt provided to the agent. 
                Specifically, we use a threshold of 100 APIs, but this could be adjusted depending on the supported language model context size.
            </p>
            <p class="text">
                <strong>Two-Stage Documentation Retrieval for Large API Sets:</strong>
                For websites with a larger number of endpoints, providing the full documentation directly within the prompt was impractical due to the size limitations of agent inputs. 
                To address this, we employed a two-stage documentation retrieval process, which allowed access to only the relevant information as needed, keeping the initial prompt concise.
                In the first stage, the user prompt could provide a description of the task, with a list of all available API endpoints along with a very brief description of each API. 
                This initial summary helps facilitating understanding the scope of the available APIs while staying within the prompt size constraints.
                In the second stage, if the model determines that it needs detailed information about a specific API endpoint or some API endpoints, it can use a tool called <code>get_api_documentation</code>. 
                This tool searches a dictionary that maps an API to its documentation, enabling the model to retrieve the full README or YAML documentation for any given endpoint by calling <code>get_api_documentation</code> with the endpoint's identifier. 
                This might include the input parameters, output formats, and examples of how to interact with the endpoint. 
                For example, to retrieve the documentation for the endpoint <code>GET /api/{id}/commits</code>, the agent would call <code>get_api_documentation("GET /api/{id}/commits")</code>, and an example returned API documentation is the documentation in <a href="#fig:api">Figure 3</a>. 
            </p>
        </div>

        <div id='hybrid' class="hybrid-block">
            <h1 class="text">Hybrid Browsing+API Calling Agents</h1>
            <p class="text">
                Then, the question arises: given the benefits of API calling, <strong>should we discard web browsing altogether?</strong> 
                The key limitation is that not all websites provide comprehensive or well-documented APIs, necessitating traditional web browsing in certain cases. 
                To address this, we propose a hybrid agent that dynamically switches between API calls and web browsing based on task requirements. 
                This agent has three options per step: communicating in natural language, generating Python code for API calls, or performing web browsing actions. 
                The hybrid agent's prompt includes both API documentation and web-browsing instructions, allowing it to adapt flexibly to each task's demands. 
                This approach improves performance by leveraging the strengths of both methods, depending on API availability and documentation quality.
            </p>
        </div>

        <div id='experiment' class="experiment-block">
            <h1 class="text">Experimental Setup</h1>
            <div id="tab:quality" style="display: flex; flex-direction: column; align-items: center;" class="figure">
                <div class="table-container">
                    <table class="data-table" style="border-collapse: collapse; width: 100%; text-align: center;">
                        <thead>
                            <tr>
                                <th style="border: 1px solid #000; padding: 8px;">Websites</th>
                                <th style="border: 1px solid #000; padding: 8px;">Gitlab</th>
                                <th style="border: 1px solid #000; padding: 8px;">Map</th>
                                <th style="border: 1px solid #000; padding: 8px;">Shopping</th>
                                <th style="border: 1px solid #000; padding: 8px;">Admin</th>
                                <th style="border: 1px solid #000; padding: 8px;">Reddit</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td style="border: 1px solid #000; padding: 8px;">Number of Endpoints</td>
                                <td style="border: 1px solid #000; padding: 8px;">988</td>
                                <td style="border: 1px solid #000; padding: 8px;">53</td>
                                <td style="border: 1px solid #000; padding: 8px;">556</td>
                                <td style="border: 1px solid #000; padding: 8px;">556</td>
                                <td style="border: 1px solid #000; padding: 8px;">31</td>
                            </tr>
                            <tr>
                                <td style="border: 1px solid #000; padding: 8px;">API Quality</td>
                                <td style="border: 1px solid #000; padding: 8px;">Good</td>
                                <td style="border: 1px solid #000; padding: 8px;">Good</td>
                                <td style="border: 1px solid #000; padding: 8px;">Fair</td>
                                <td style="border: 1px solid #000; padding: 8px;">Fair</td>
                                <td style="border: 1px solid #000; padding: 8px;">Poor</td>
                            </tr>
                            <tr>
                                <td style="border: 1px solid #000; padding: 8px;">Documentation Quality</td>
                                <td style="border: 1px solid #000; padding: 8px;">Good</td>
                                <td style="border: 1px solid #000; padding: 8px;">Good</td>
                                <td style="border: 1px solid #000; padding: 8px;">Fair</td>
                                <td style="border: 1px solid #000; padding: 8px;">Fair</td>
                                <td style="border: 1px solid #000; padding: 8px;">Poor</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
            <figcaption style="text-align: center; width: 100%;">
                Table 1: Number of endpoints, quality of API, and documentation quality for WebArena websites.
            </figcaption>
            
            <p class="text">
                Here, we provide a detailed analysis of the API support for various websites used in the WebArena tasks, categorized into three levels: good, medium, and poor. 
                The availability, functionality, and documentation of APIs, as described in <a href="#tab:quality">Table 1</a>, play a crucial role in the efficiency and flexibility of our agents.
            </p>

        </div>

        <div id='eval' class="eval-block">
            <h1 class="text">Evaluation</h1>

            
            <p class="text">
                For evaluation, we compare Pangea-7B against several state-of-the-art open source baselines, including English-centric models Llava-1.5-7B <d-cite key="liu2023improvedllava"></d-cite>, Llava-Next-7B <d-cite key="liu2024llavanext"></d-cite>, Phi-3.5-Vision <d-cite key="abdin2024phi3technicalreporthighly"></d-cite>, Cambrian-8B <d-cite key="tong2024cambrian"></d-cite> and multilingual models PaliGemma-3B <d-cite key="beyer2024paligemma"></d-cite>, PALO-7B <d-cite key="PALO"></d-cite>, mBLIP mT0-XL and mBLIP BLOOMZ <d-cite key="geigle_etal_2024_mblip"></d-cite>. 
                We also consider two text-only LLMs baselines Vicuna-1.5-7B <d-cite key="zheng2023judging"></d-cite> and Qwen2-7B-Instruct <d-cite key="yang2024qwen2"></d-cite>, which are the backbones of Llava-Next and our Pangea-7B respectively. 
                We integrate our multimodal tasks in PangeaBench into <a href="https://github.com/EvolvingLMMs-Lab/lmms-eval" target="_blank">lmms-eval</a> <d-cite key="lmms_eval2024"></d-cite>, a multimodal evaluation package that supports many English multimodal benchmarks. 
                We use <a href="https://github.com/EleutherAI/lm-evaluation-harness" target="_blank">lm-evaluation-harness</a> <d-cite key="biderman2024lessons"></d-cite> to evaluate text-only tasks. 
                We follow the original paper for their best models' prompts in different tasks.
                <a href="#fig:teaser">Figure 1</a> shows the aggregate performance of various multimodal LLMs on PangeaBench.
            </p>

            <div id="tab:final_table" style="display: flex; flex-direction: column; align-items: center;" class="figure">
                <div class="table-container">
                    <table class="data-table">
                        <thead>
                            <tr>
                                <th rowspan="3">Models</th>
                                <th colspan="2">AVG (all)</th>
                                <th colspan="4">Multimodal Chat</th>
                                <th colspan="4">Cultural Understanding</th>
                                <th colspan="4">Captioning</th>
                                <th colspan="4">Short VQA</th>
                                <th colspan="4">Multi-subject Reasoning</th>
                            </tr>
                            <tr>
                                <th colspan="2">AVG (all)</th>
                                <th colspan="2">xChatBench</th>
                                <th colspan="2">M-LlavaBench</th>
                                <th colspan="2">CVQA</th>
                                <th colspan="2">MaRVL</th>
                                <th colspan="2">XM100</th>
                                <th colspan="2">xGQA</th>
                                <th colspan="2">MaXM</th>
                                <th colspan="2">xMMMU</th>
                                <th colspan="2">M3Exam</th>
                            </tr>
                            <tr>
                                <th>en</th>
                                <th>mul</th>
                                <th>en</th>
                                <th>mul</th>
                                <th>en</th>
                                <th>mul</th>
                                <th>en</th>
                                <th>mul</th>
                                <th>en</th>
                                <th>mul</th>
                                <th>en</th>
                                <th>mul</th>
                                <th>en</th>
                                <th>mul</th>
                                <th>en</th>
                                <th>mul</th>
                                <th>en</th>
                                <th>mul</th>
                                <th>en</th>
                                <th>mul</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr class="highlight-gray">
                                <td colspan="22"><i>Proprietary Models</i></td>
                            </tr>
                            <tr>
                                <td>Gemini-1.5-Pro</td>
                                <td>67.1</td>
                                <td>62.5</td>
                                <td>67.0</td>
                                <td>54.4</td>
                                <td>103.4</td>
                                <td>106.6</td>
                                <td>75.9</td>
                                <td>75.7</td>
                                <td>76.4</td>
                                <td>72.0</td>
                                <td>27.6</td>
                                <td>19.1</td>
                                <td>54.2</td>
                                <td>48.7</td>
                                <td>56.4</td>
                                <td>63.5</td>
                                <td>65.8</td>
                                <td>57.7</td>
                                <td>77.4</td>
                                <td>64.7</td>
                            </tr>
                            <tr>
                                <td>GPT4o</td>
                                <td>68.6</td>
                                <td>64.6</td>
                                <td>71.0</td>
                                <td>64.4</td>
                                <td>104.6</td>
                                <td>100.4</td>
                                <td>79.1</td>
                                <td>79.4</td>
                                <td>81.4</td>
                                <td>82.1</td>
                                <td>27.7</td>
                                <td>19.1</td>
                                <td>55.8</td>
                                <td>51.0</td>
                                <td>60.7</td>
                                <td>65.4</td>
                                <td>69.1</td>
                                <td>58.3</td>
                                <td>68.0</td>
                                <td>61.0</td>
                            </tr>
                            <tr class="highlight-gray">
                                <td colspan="22"><i>English Models</i></td>
                            </tr>
                            <tr>
                                <td>Llava-1.5-7B</td>
                                <td>45.4</td>
                                <td>28.4</td>
                                <td>28.5</td>
                                <td>11.8</td>
                                <td>66.1</td>
                                <td>40.8</td>
                                <td>48.9</td>
                                <td>36.5</td>
                                <td>56.2</td>
                                <td>53.7</td>
                                <td>28.6</td>
                                <td>1.1</td>
                                <td>62.0</td>
                                <td>30.6</td>
                                <td>49.8</td>
                                <td>20.4</td>
                                <td>36.2</td>
                                <td>31.5</td>
                                <td>32.3</td>
                                <td>29.0</td>
                            </tr>
                            <tr>
                                <td>Llava-Next-7B</td>
                                <td>51.1</td>
                                <td>32.7</td>
                                <td>40.5</td>
                                <td>18.9</td>
                                <td>78.9</td>
                                <td>50.7</td>
                                <td>55.7</td>
                                <td>42.6</td>
                                <td>62.8</td>
                                <td>50.9</td>
                                <td>29.3</td>
                                <td>9.4</td>
                                <td>64.8</td>
                                <td>37.8</td>
                                <td>54.9</td>
                                <td>21.4</td>
                                <td>36.7</td>
                                <td>34.3</td>
                                <td>36.5</td>
                                <td>28.4</td>
                            </tr>
                            <tr>
                                <td>Phi-3.5-Vision</td>
                                <td>54.0</td>
                                <td>35.0</td>
                                <td>38.5</td>
                                <td>13.2</td>
                                <td>70.8</td>
                                <td>58.0</td>
                                <td>56.3</td>
                                <td>42.3</td>
                                <td>72.1</td>
                                <td>56.5</td>
                                <td>30.2</td>
                                <td>5.2</td>
                                <td>64.7</td>
                                <td>38.4</td>
                                <td>55.3</td>
                                <td>25.0</td>
                                <td>42.6</td>
                                <td>38.8</td>
                                <td>55.8</td>
                                <td>37.2</td>
                            </tr>
                            <tr>
                                <td>Cambrian-8B</td>
                                <td>50.9</td>
                                <td>36.4</td>
                                <td>27.5</td>
                                <td>11.3</td>
                                <td>78.4</td>
                                <td>61.8</td>
                                <td>59.7</td>
                                <td>47.5</td>
                                <td>75.4</td>
                                <td>61.8</td>
                                <td>20.6</td>
                                <td>9.9</td>
                                <td>64.6</td>
                                <td>39.8</td>
                                <td>55.3</td>
                                <td>28.7</td>
                                <td>41.8</td>
                                <td>33.2</td>
                                <td>34.7</td>
                                <td>33.4</td>
                            </tr>
                            <tr>
                                <td>LLaVA-OV-7B</td>
                                <td>59.5</td>
                                <td>41.3</td>
                                <td>51.0</td>
                                <td>28.5</td>
                                <td>89.7</td>
                                <td>55.3</td>
                                <td>65.2</td>
                                <td>53.7</td>
                                <td>72.7</td>
                                <td>57.5</td>
                                <td>30.6</td>
                                <td>7.0</td>
                                <td>64.4</td>
                                <td>48.2</td>
                                <td>54.9</td>
                                <td>34.8</td>
                                <td>46.3</td>
                                <td>41.0</td>
                                <td>60.4</td>
                                <td>45.8</td>
                            </tr>
                            <tr>
                                <td>Molmo-7B-D</td>
                                <td>55.4</td>
                                <td>34.1</td>
                                <td>49.5</td>
                                <td>21.1</td>
                                <td>95.9</td>
                                <td>13.8</td>
                                <td>59.4</td>
                                <td>48.3</td>
                                <td>65.3</td>
                                <td>54.9</td>
                                <td>22.1</td>
                                <td>9.1</td>
                                <td>51.5</td>
                                <td>43.0</td>
                                <td>52.9</td>
                                <td>37.5</td>
                                <td>44.5</td>
                                <td>40.4</td>
                                <td>57.1</td>
                                <td>39.1</td>
                            </tr>  
                            <tr>
                                <td>Llama3.2-11B</td>
                                <td>57.2</td>
                                <td>41.9</td>
                                <td>49.0</td>
                                <td>27.8</td>
                                <td>93.9</td>
                                <td>58.2</td>
                                <td>70.2</td>
                                <td>61.4</td>
                                <td>64.5</td>
                                <td>58.1</td>
                                <td>27.6</td>
                                <td>4.5</td>
                                <td>55.6</td>
                                <td>45.4</td>
                                <td>55.3</td>
                                <td>43.9</td>
                                <td>46.5</td>
                                <td>41.4</td>
                                <td>51.8</td>
                                <td>36.6</td>
                            </tr>
                            <tr class="highlight-gray">
                                <td colspan="22"><i>Multilingual Models</i></td>
                            </tr>
                            <tr>
                                <td>PaliGemma-3B</td>
                                <td>37.3</td>
                                <td>25.8</td>
                                <td>6.0</td>
                                <td>3.5</td>
                                <td>32.1</td>
                                <td>31.9</td>
                                <td>52.9</td>
                                <td>42.9</td>
                                <td>56.5</td>
                                <td>52.2</td>
                                <td>18.7</td>
                                <td>0.8</td>
                                <td>59.7</td>
                                <td>30.5</td>
                                <td>47.9</td>
                                <td>19.9</td>
                                <td>26.3</td>
                                <td>25.2</td>
                                <td>36.0</td>
                                <td>25.6</td>
                            </tr>
                            <tr>
                                <td>PALO-7B</td>
                                <td>46.3</td>
                                <td>32.2</td>
                                <td>27.0</td>
                                <td>11.8</td>
                                <td>68.9</td>
                                <td>71.2</td>
                                <td>50.9</td>
                                <td>39.2</td>
                                <td>63.3</td>
                                <td>54.2</td>
                                <td>30.4</td>
                                <td>0.8</td>
                                <td>60.5</td>
                                <td>37.8</td>
                                <td>51.4</td>
                                <td>16.3</td>
                                <td>33.1</td>
                                <td>30.5</td>
                                <td>30.8</td>
                                <td>27.8</td>
                            </tr>
                            <tr>
                                <td>mBLIP mT0-XL</td>
                                <td>35.1</td>
                                <td>29.8</td>
                                <td>2.5</td>
                                <td>0.5</td>
                                <td>32.7</td>
                                <td>28.2</td>
                                <td>40.5</td>
                                <td>37.5</td>
                                <td>67.3</td>
                                <td>66.7</td>
                                <td>31.9</td>
                                <td>3.1</td>
                                <td>44.2</td>
                                <td>39.9</td>
                                <td>44.7</td>
                                <td>36.8</td>
                                <td>29.3</td>
                                <td>30.4</td>
                                <td>22.8</td>
                                <td>25.0</td>
                            </tr>
                            <tr>
                                <td>mBLIP BLOOMZ</td>
                                <td>36.1</td>
                                <td>30.0</td>
                                <td>4.0</td>
                                <td>1.6</td>
                                <td>43.5</td>
                                <td>41.0</td>
                                <td>44.9</td>
                                <td>36.9</td>
                                <td>62.3</td>
                                <td>58.6</td>
                                <td>22.5</td>
                                <td>10.3</td>
                                <td>43.3</td>
                                <td>36.9</td>
                                <td>44.7</td>
                                <td>24.8</td>
                                <td>29.2</td>
                                <td>30.8</td>
                                <td>30.3</td>
                                <td>29.5</td>
                            </tr>
                            <tr class="highlight-gray">
                                <td colspan="22"><i>Pangea</i></td>
                            </tr>
                            <tr>
                                <td>Pangea-7B (Ours)</td>
                                <td>59.9</td>
                                <td>52.7</td>
                                <td>46.0</td>
                                <td>35.6</td>
                                <td>84.2</td>
                                <td>89.5</td>
                                <td>64.4</td>
                                <td>57.2</td>
                                <td>87.0</td>
                                <td>79.0</td>
                                <td>30.4</td>
                                <td>14.2</td>
                                <td>64.7</td>
                                <td>60.2</td>
                                <td>55.3</td>
                                <td>53.2</td>
                                <td>45.7</td>
                                <td>43.7</td>
                                <td>61.4</td>
                                <td>42.1</td>
                            </tr>
                            <tr>
                                <td>Δ over SoTA Open</td>
                                <td>+0.4</td>
                                <td>+10.8</td>
                                <td>-3.5</td>
                                <td>+7.1</td>
                                <td>-11.7</td>
                                <td>+18.3</td>
                                <td>-5.8</td>
                                <td>-4.2</td>
                                <td>+11.6</td>
                                <td>+12.3</td>
                                <td>-0.2</td>
                                <td>+3.9</td>
                                <td>-0.1</td>
                                <td>+12.0</td>
                                <td>0.0</td>
                                <td>+9.3</td>
                                <td>-0.8</td>
                                <td>+2.3</td>
                                <td>+1.0</td>
                                <td>-3.7</td>
                            </tr>
                                                  
                        </tbody>
                    </table>
                </div>
            </div>
            
            <figcaption style="text-align: center; width: 100%;">
                Table 1: Models' multilingual multimodal evaluation results on PangeaBench.
            </figcaption>

            <p class="text">
                <strong>Multilingual Multimodal Results</strong>
                We show the performance of models on the multimodal tasks from PangeaBench in <a href="#tab:final_table">Table 1</a>.
                The results provide clear insights into the strengths and remaining challenges of Pangea-7B in multilingual and multimodal tasks. Key observations from the evaluation include:
                <br>
                1) Superior English and Multilingual Performance: Pangea-7B outperforms existing open-source models across both English and multilingual tasks. Particularly in cultural understanding (CVQA, MaRVL), it has achieved substantial gains, highlighting its effectiveness in both cross-lingual and cross-cultural contexts.
                <br>
                2) Balanced Cross-Language Capabilities: Unlike many models that exhibit a significant drop in performance when moving from English to multilingual tasks, Pangea-7B is relatively consistent. For instance, in Multimodal Chat tasks, the performance gap between English and multilingual remains relatively small, indicating its ability to handle multiple languages effectively.
                <br>
                3) Challenges Compared to Proprietary Models: While Pangea-7B leads in open-source models, some gaps remain when compared to closed-source models like GPT4o. Additionally, though Pangea-7B narrows the gap between English and multilingual performance, there is still room for improvement in fully closing this divide across all tasks.
            </p>

            <p class="text">
                <strong>Multilingual Text-only Results</strong>
                We further evaluate our model in text-only scenarios in <a href="#tab:text_table">Table 2</a>. Interesting findings include:
                <br>
                1) Best Text Performance Among Multimodal LLMs: Pangea-7B demonstrates the strongest performance among all multimodal LLMs in the text-only tasks consistently outperforming baselines like Llava-Next-7B. This highlights that, despite being trained as a multimodal model, Pangea-7B maintains superior text understanding and reasoning capabilities compared to other MLLMs.
                <br>
                2) Maintained Performance from its Text Backbone:  Pangea-7B generally maintains or sees slight drops in performance on most text-only benchmarks compared with its text backbone Qwen2-7B-Instruct. Notably, the model shows a significant improvement in MGSM. This improvement is directly attributable to the inclusion of math-related instructions in PangeaIns, which enhances the model's capability to handle complex multilingual reasoning and mathematical tasks.
            </p>

            
            <figcaption style="text-align: center; width: 100%;">
                Table 2: Models' multilingual text-only evaluation results on PangeaBench.
            </figcaption>
        

        </div>

        <div id='discussion' class="discussion-block">
            <h1 class="text">Discussion</h1>

            <d-figure id="fig:scaling_effect">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/scaling_effect.png" alt="scaling effect">
                    <figcaption>
                        <strong>Figure 5:</strong> Scaling effect of training samples on English and multilingual scores across datasets.
                    </figcaption>
                </figure>
            </d-figure>

            <p class="text">
                <strong>Scaling Effect of Number of Instructions:</strong>
                Understanding how the quantity of instructions affects model performance is crucial for optimizing training strategies and resource allocation. 
                <a href="#fig:scaling_effect">Figure 5</a> reveals a clear scaling effect related to the number of instructions used during training. 
                Performance improvements were consistent as we increased the number of multilingual instructions in PangeaIns, for both English and multilingual performance. 
                This demonstrates the necessity of scaling multilingual multimodal instruction tuning. 
            </p>

            <d-figure id="fig:english_ratio" style="display: flex; justify-content: center;">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/english_vs_multilingual.png" alt="english_vs_multilingual" style="width: 60%" class="center">
                    <figcaption>
                        <strong>Figure 6:</strong> Impact of English training data proportion on English vs. multilingual performance.
                    </figcaption>
                </figure>
            </d-figure>

            <p class="text">
                <strong>Role of English Data:</strong>
                In multilingual scenarios, English data plays a pivotal role in cross-lingual transfer. 
                To investigate this, we sampled 500K examples from the translated data described in <a href="#machine_translation">Machine Translated Instructions</a>, ensuring a consistent data distribution. 
                We varied the ratio of English data while keeping the total number of training samples fixed at 500K. 
                For the 17 multilingual languages in the translated subset, we evenly distributed the number of samples across languages. 
                As shown in <a href="#fig:english_ratio">Figure 6</a>, English performance generally improves as the percentage of English data increases. 
                More surprisingly, using no English data (full multilingual data) results in relatively lower multilingual performance. 
                As we introduce more English data, multilingual performance improves, peaking at 38.7% with 40% English. 
                However, performance drops sharply when English data reaches 100%. 
                This suggests that English data aids cross-lingual transfer, however, over-reliance on it harms multilingual performance. 
            </p>
        </div>

        <d-figure id="fig:language_portion_downstream_performance">
            <figure>
                <img data-zoomable="" draggable="false" src="static/img/train_eval_perf_percentage.png" alt="train_eval_perf_percentage">
                <figcaption>
                    <strong>Figure 7:</strong> The relationship between training sample size (relative to English) and performance (relative to English) of different languages across four datasets.
                </figcaption>
            </figure>
        </d-figure>

        <p class="text">
            <strong>How does the proportion of training samples in a language affect downstream performance?</strong>
            An interesting question to ask is whether the downstream task performance is correlated with the number of training samples. 
            Our analysis in <a href="#fig:language_portion_downstream_performance">Figure 7</a> revealed a nuanced relationship between training sample proportion and downstream performance. 
            While there is a general positive correlation, the impact varies significantly across languages and tasks. 
            For widely spoken languages with rich linguistic resources, we observed a near-linear relationship. 
            However, for low-resource languages, even a small increase in proportion yielded disproportionately large performance gains. 
            Interestingly, we also noted instances of positive transfer between typologically similar languages. 
            These findings suggest that strategic allocation of training samples, considering both language prevalence and linguistic similarities, can optimize overall model performance.
        </p>

        <d-figure id="fig:ocr_accuracy" style="display: flex; justify-content: center;">
            <figure>
                <img data-zoomable="" draggable="false" src="static/img/ocr_accuracy.png" alt="ocr_accuracy" style="width: 60%" class="center">
                <figcaption>
                    <strong>Figure 8:</strong> A preliminary exploration of multilingual OCR.
                </figcaption>
            </figure>
        </d-figure>

        <p class="text">
            <strong>Preliminary Explorations of Multilingual OCR:</strong>
            Multilingual OCR emerged as a particularly challenging aspect of Pangea's functionality. 
            We made efforts to enhance its multilingual OCR capabilities. 
            Specifically, we constructed a dataset of 500K multilingual OCR instructions spanning 10 languages, with 50K examples per language, sourced from web user interfaces. 
            Webpages naturally serve as image-rich environments containing abundant text, and by capturing screenshots of websites from various countries in different languages, we were able to gather a substantial number of OCR images. 
            We employed the same model architecture as Pangea but trained it exclusively on these OCR images, reserving a portion of the data as a test set. As shown in <a href="#fig:ocr_accuracy">Figure 8</a>, the results indicate that improving multilingual OCR performance is feasible with an increase in training samples. 
            However, the OCR accuracy for non-Latin scripts (e.g., Chinese, Japanese, and Korean) remains lower than for Latin-based languages. 
            Looking ahead, we aim to further expand the multilingual OCR training dataset to include more languages and integrate this data into PangeaIns.
        </p>

    </div>

        <div id="conclusion" style="position: relative; margin-top: 40px; margin-bottom: 0px;">
            <h2 class="text" style="margin-top:0px; margin-bottom:10px">Conclusion</h2>
            <p class="text">
                We introduced Pangea, a novel multilingual multimodal large language model designed to bridge linguistic and cultural gaps in visual understanding tasks. 
                By leveraging PangeaIns, our newly curated 6M multilingual multimodal instruction data samples, we demonstrated significant improvements in cross-lingual and cross-cultural understanding across 39 typologically diverse languages. 
                Our comprehensive evaluation using PangeaBench revealed Pangea's superior performance compared to existing open-source models, particularly in tasks requiring nuanced cultural interpretation. 
                We also highlight ongoing challenges in areas such as low-resource language support and multilingual OCR. 
                We fully open-source Pangea, PangeaIns, and PangeaBench to facilitate future research to build open and inclusive MLLMs. 
            </p>
        </div>

        <div id="acknowledgement" style="position: relative; margin-top: 40px; margin-bottom: 0px;">
            <h2 class="text" style="margin-top:0px; margin-bottom:10px">Acknowledgement</h2>
            <p class="text">
                This work was supported in part by the Carnegie Bosch Institute Fellowship and a grant from DSTA Singapore.  
                The training is supported by the CMU FLAME Center. 
                The authors would like to thank CMU NeuLab colleagues for their constructive comments. 
                The authors would like to thank Google Gemini credits for data construction and evaluation.
                The authors would like to thank Cambrian team for their <a href="https://cambrian-mllm.github.io/" target="_blank">project webpage template</a>. 
            </p>
        </div>

        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @article{tong2024cambrian,<br>
                &nbsp;&nbsp;title={{Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs}},<br>
                &nbsp;&nbsp;author={Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng, and Iyer, Adithya and Pan, Xichen and Wang, Austin and Fergus, Rob and LeCun, Yann and Xie, Saining},<br>
                &nbsp;&nbsp;journal={arXiv preprint arXiv:2406.16860},<br>
                &nbsp;&nbsp;year={2024}<br>
                }
            </p>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>   
        <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
        <d-bibliography src="bibliography.bib"></d-bibliography>
        <script src="./static/js/nav-bar.js"></script>
    </body>
</html>
