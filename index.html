<!doctype html>
<html lang="en">
    <head>
        <title>Beyond Browsing: API-Based Web Agents</title>
        <link rel="icon" type="image/x-icon" href="/static/img/icons/web.png">

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Open Graph -->
        <meta property="og:url" content="https://github.com/yueqis/API-Based-Agent" />
        <meta property="og:image" content="https://pangea.github.io/static/img/preview.png" />
        <meta property="og:title" content="Beyond Browsing: API-Based Web Agents" />
        <meta property="og:description" content="Beyond Browsing: API-Based Web Agents" />
        
        <!-- Twitter -->
        <meta name="twitter:url" content="https://github.com/yueqis/API-Based-Agent" />
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:image" content="https://github.com/yueqis/API-Based-Agent/static/img/preview.png" />
        <meta name="twitter:title" content="Beyond Browsing: API-Based Web Agents" />
        <meta name="twitter:description" content="This project explores a novel approach to web agents by enabling them to use APIs in addition to traditional web-browsing techniques. By leveraging API calls, agents can perform tasks more efficiently and accurately, especially on websites with comprehensive API support." />
       
        <script src="./static/js/distill_template.v2.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

        <script defer="" src="./static/js/hider.js"></script>
        <script src="./static/js/image_interact.js"></script>
        <script src="./static/js/switch_videos.js"></script>

        <link rel="stylesheet" href="./static/css/style.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>


        <!-- medium zoom https://github.com/francoischalifour/medium-zoom -->
        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>  <!-- jquery -->
        <script defer src="./static/js/medium-zoom.min.js"></script>
        <script defer src="./static/js/zoom.js"></script>
    </head>
    <body>
        <div class="header-wrapper">
            <div class="header-container" id="header-container">
                <div class="header-content">
                    <h1 style="margin-top: 0px"><i>Beyond Browsing: API-Based Web Agents</i></h1>
                        <p>
                            This project explores a novel approach to web agents by enabling them to use <strong>APIs</strong> in addition to traditional web-browsing techniques. 
                            By leveraging API calls, agents can perform tasks more efficiently and accurately, especially on websites with comprehensive API support.
                        </p>
                    
                        <div class="icon-container">
                            <div class="icon-item">
                                <img src="./static/img/icons/api.png" alt="api Icon">
                                <div><strong>API-Based Agent</strong>: The API-based agent leverages application programming interfaces (APIs) to interact directly with web services, bypassing traditional web-browsing actions like simulated clicks.</div>
                            </div>
                            <div class="icon-item">
                                <img src="./static/img/icons/hybrid.png" alt="hybrid Icon">
                                <div><strong>Hybrid Agent</strong>: a agent that combines the power of API-Based Agent and traditional Web-Based Agent, capable of interleaving API calls and Web Browsing.</div>
                            </div>
                            <div class="icon-item">
                                <img src="./static/img/icons/task.png" alt="Benchmarking Icon">
                                <div><strong>Real-World Web Task Evaluation and Analysis</strong>: On WebArena, a real-world web task benchmark, our hybrid agent achieve sota performance among task-agnostic models.</div>
                            </div>
                        </div>

                    <div class="button-container">
                        <!-- replace arxiv -->
                        <a href="https://arxiv.org/abs/2406.16860" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="ai ai-arxiv"></i>
                            </span>
                            arXiv
                        </a>
                        <!-- replace pdf -->
                        <a href="https://arxiv.org/pdf/2406.16860" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="fas fa-file-pdf"></i>
                            </span>
                            <span>pdf</span>
                        </a>
                        <!-- replace image -->
                        <a href="https://github.com/yueqis/API-Based-Agent" class="button" target="_blank">
                            <span class="icon is-small">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>Code</span>
                        </a>
                        <!-- <br> -->                      
                    </div>
                </div>
                <div class="header-image">
                    <img draggable="false" src="static/img/icons/agent.png" alt="Teaser Image" class="teaser-image">
                </div>
            </div>
        </div>
    <d-article>
        <div class="byline">
            <div class="byline-container">
                <p>
                    <a href="https://yueqis.github.io/" class="author-link" target="_blank">Yueqi Song</a> &emsp;
                    <a href="https://frankxfz.me/" class="author-link" target="_blank">Frank Xu</a> &emsp;
                    <a href="https://shuyanzhou.github.io/" class="author-link" target="_blank">Shuyan Zhou</a> &emsp;
                    <a href="https://www.phontron.com/" class="author-link" target="_blank">Graham Neubig</a>
                    <p></p>
                    <a href="https://www.cs.cmu.edu/" class="affiliation-link" id="affiliation" target="_blank">Carnegie Mellon University</a>
                </p>
            </div>
        </div>
        
        <p class="text abstract">

            Web browsers are a portal to the internet, where much of human activity is undertaken. 
            Thus, there has been significant research work in AI agents that interact with the internet through web browsing.
            However, there is also another interface designed specifically for machine interaction with online content: application programming interfaces (APIs).
            In this paper we ask -- <strong>what if we were to take tasks traditionally tackled by browsing agents, and give AI agents access to APIs?</strong>
            To do so, we propose two varieties of agents: (1) an API-calling agent that attempts to perform online tasks through APIs only, similar to traditional coding agents, and (2) a hybrid agent that can interact with online data through both web browsing and APIs.
            In experiments on WebArena <d-cite key="zhou2023webarena"></d-cite>, a widely-used and realistic benchmark for web navigation tasks, we find that API-based agents outperform web browsing agents, as depicted in <a href="#fig:main">Figure 1</a>.
            Hybrid agents outperform both others nearly uniformly across tasks, resulting in a more than 20.0% absolute improvement over web browsing alone, achieving a success rate of 35.8%.
            These results strongly suggest that when APIs are available, they present an attractive alternative to relying on web browsing alone.
        </p>

        <figure id="fig:main">
            <img data-zoomable="" draggable="false" src="static/img/main.png" alt="main">
            <figcaption>
                <strong>Figure 1:</strong> A comparison of three types of agents. 
                The Browsing Agent performs tasks through web browsing only, utilizing the accessibility tree to interact with web pages, achieving an average performance of 14.8% on WebArena. 
                The API-Based Agent performs tasks by making API calls and generating code without relying on web browsing, achieving an average accuracy of 29.2%. 
                The Hybrid Agent combines both methods, dynamically switching between web browsing and API calling, depending on the task. 
                This allows the execution of either API calls or web browsing actions, or both in combination, improving performance by more than 5 percentage points compared to the API-Based Agent.
            </figcaption>
        </figure>

        <p class="text abstract">
            This project is structured around three key aspects:
            <ol class="text">
                <li><strong><a href="#api">&sect;API-Based Agent</a></strong>: The API-based agent is designed to interact directly with web services using structured API calls, bypassing traditional web-browsing methods like simulated clicks and form inputs. By leveraging predefined endpoints, the agent can efficiently retrieve and manipulate data, reducing the number of steps required to complete tasks. This approach not only improves task accuracy but also enhances efficiency, especially on websites with comprehensive API support.</li>
                <li><strong><a href="#hybrid">&sect;Hybrid Agent</a></strong>: The hybrid agent combines the strengths of API-based interactions and traditional web browsing by dynamically switching between the two methods based on the task requirements. This flexibility allows the agent to leverage API calls when they are available and efficient, while seamlessly resorting to web browsing actions for tasks that lack adequate API support. As a result, the hybrid agent is capable of handling a wider range of tasks with improved accuracy and efficiency compared to API-Based and traditional web agents. </li>
                <li><strong><a href="#benchmarking">&sect;Real-World Task Evaluation and Analysis</a></strong>: Are compare the agent on the WebArena benchmark. We are the first to perform a comparison of API-based agents, browsing-only agents, and hybrid agents. The results demonstrated that API-based agents outperformed browsing-only agents on websites with comprehensive API support, while the hybrid agent achieved the highest overall accuracy by dynamically switching between APIs and web browsing. Our analysis shows that the hybrid approach not only improves task efficiency but also provides greater flexibility and robustness in handling diverse and complex web interactions.</li>
            </ol>
        </p>

        <div class="icon-row">
            <a href="#browsing" class="icon-link">
                <img src="static/img/icons/web.png" alt="browsing Logo" class="icon">
                Browsing Tasks
            </a>
            <a href="#api" class="icon-link">
                <img src="static/img/icons/api.png" alt="api Logo" class="icon">
                API-Based Agent
            </a>
            <a href="#hybrid" class="icon-link">
                <img src="static/img/icons/hybrid.png" alt="hybrid Logo" class="icon">
                Hybrid Agent
            </a>
            <a href="#experiment" class="icon-link">
                <img src="static/img/icons/task.png" alt="task Logo" class="icon">
                Experiments
            </a>
            <a href="#results" class="icon-link">
                <img src="static/img/icons/result.png" alt="result Logo" class="icon">
                Results and Analysis
            </a>
        </div>

        <p class="click-hint" style="width: 85%;">
            <img src="static/img/icons/click.gif" style="width: 1.5rem">
            <strong>Click to jump to each section.</strong>
        </p>
        
        <hr>

        <div id='browsing' class="browsing-block">
            <h1 class="text">Background: Web Browsing</h1>
            <h3 class="text">The Web Browsing Task</h3>
            <p class="text">
                we focus on WebArena tasks, which simulate real-world scenarios to evaluate an agent's ability to complete diverse web-based activities.
                Tasks in WebArena include interacting with platforms like Gitlab (to manage projects and repositories), Reddit (to browse and post content), e-commerce websites (for shopping), and mapping services (for trip planning).
            </p>
            <h3 class="text">Web Browsing Agent</h3>
            <p class="text">
                A baseline web browsing agent leverages the accessibility tree of web pages, which organizes interactive elements like buttons, input fields, and links in a hierarchical structure. 
                This structure makes it easier for agents to navigate the web by simulating human-like browsing behaviors such as clicking, filling out forms, and moving between pages. 
                The agent maintains a comprehensive history of all its previous actions, allowing it to inform its future decisions based on past interactions. 
                However, due to the complexity of some web elements and their dynamic nature, the browsing agent struggles with tasks requiring numerous or intricate interactions. 
                For example, in the task in <a href="#fig:example">Figure 2</a>, the agent needs to determine the number of commits made by a specific user to a project. 
                A traditional browsing-based approach involves logging in, navigating to the correct project, and attempting to scroll through and find the user's commits, and thus the task becomes too complex and fails after the agent's 15-step limit.
            </p>
            <figure id="fig:example">
                <img data-zoomable="" draggable="false" src="static/img/example.png" alt="example" style="width: 80%" class="center">
                <figcaption>
                    <strong>Figure 2:</strong> The API-based agent can often solve problems in many fewer function calls than traditional browsing agents. 
                    In this task, web browsing failed to solve the intent "find the number of commits the user `SaptakS` made to the repo `a11yproject`" after 15 steps, while our API-based agent successfully completed the task with only three lines of code.
                </figcaption>
            </figure>        
        </div>

        <div id="instruction_data" class="data-block">
            <h1 class="text">PangeaIns</h1>
            <p class="text">
                Creating a truly multilingual, multicultural MLLM presents unique challenges. 
                We developed PangeaIns, a diverse and high-quality dataset specifically designed for instruction tuning. 
                PangeaIns features an extensive and balanced distribution of languages, tasks, and cultural contexts. 
                Comprising 6 million samples in 39 languages, PangeaIns was curated with a focus on linguistic and cultural diversity. 
                We empirically keep the final language ratio of English to Multilingual as 40%:60% as we found a significant portion of English data plays an important role in cross-lingual transfer. This is discussed in more details in (see <a href="#discussion">Discussion</a>).
                <a href="#fig:train_data_distribution">Figure 2</a> shows the details of our PangeaIns's distribution. 
                We implemented three key strategies to ensure comprehensive coverage, each addressing the specific hurdles encountered in multilingual multimodal learning. 
            </p>
    
            <div class="subsection">
                <h3 class="text">Machine Translated Instructions</h3>
                <p class="text" id="machine_translation">
                    We first create a high-quality set of English multimodal instructions, which serve as the foundation for translation into other languages.
                    <a href="#fig:train_data_distribution">Figure 2</a> shows the statistics of our translated datasets.
                    Then, we use the proprietary model Gemini 1.5 Pro <d-cite key="deepmind_gemini_report"></d-cite> to translate the English instructions into 17 languages.
                    Lastly, we developed a post-processing pipeline. This pipeline automatically corrected these errors or directly dropped the examples, ensuring that all translated instructions remained consistent.
                </p>
            </div>
            <div class="subsection">
                <h3 class="text">Multicultural Understanding Instructions</h3>
                <p class="text">
                    While machine translation enables us to scale across multiple languages, data translated from English is still Anglo-centric in its coverage of cultural <d-cite key="yu-etal-2022-beyond"></d-cite>.
                    To address this, we developed a pipeline focused on creating instructions for multicultural understanding. 
                    The pipeline of creating multicultural understanding instructions is shown in <a href="#fig:cultural_understanding_pipeline">Figure 3</a>.
                </p>
                <d-figure id="fig:cultural_understanding_pipeline">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/cultural_data_gen_pipeline.png" alt="Overview of multicultural understanding instructions data generation pipeline.">
                        <figcaption>
                            <strong>Figure 3:</strong> Overview of multicultural understanding instructions data generation pipeline.
                        </figcaption>
                    </figure>
                </d-figure>
    
                <p class="text">
                    <strong>Curation of Culturally Diverse Images.</strong> 
                    We began by sampling 10 million images from the LAION-Multi dataset <d-cite key="schuhmann2022laion"></d-cite>.
                    1) Heuristic Filtering: We implemented automatic filtering based on several key criteria: Image Size, Aspect Ratio, Text Length, NSFW content, Offensive Text, Deduplication, and CLIP Score (used to assess the alignment between the image and its textual description). This helped remove low-quality or inappropriate images and ensured the remaining dataset adhered to quality standards.
                    2) LLM Scoring: we employed Llama-3.1-8B-Instruct <d-cite key="dubey2024llama"></d-cite> to evaluate the relevance and quality of the accompanying text descriptions (alt text) for each image. The following tasks are evaluated by the model: text quality, subject classificationt, country/region classification (images classified as `no specific country` were excluded).
                    3) Avoiding Overrepresentation: We downsampled images from frequently occurring subjects.
                    Ultimately, we curated a final set of 1 million high-quality, culturally specific images that form the foundation of our dataset.
                </p>
                
                <p class="text">
                    <strong>Captioning of Multicultural Images with Different Languages.</strong> 
                    To provide context and enhance the models' ability to interpret the images accurately, we regenerated a more detailed caption using Gemini 1.5 Pro based on high-quality original text. Each image was accompanied by a caption written in the language corresponding to its cultural origin.
                </p>

                <p class="text">
                    <strong>Generating Multilingual and Cross-Cultural Instructions.</strong> 
                    For each image, we used Gemini 1.5 Pro to generate captions in native languages, leveraging high-quality alt text to enrich context. 
                    This alt text provided crucial cultural and contextual information, such as identifying key figures or locations. We carefully engineered prompts to create multilingual instructions based on 13 task types like Information Seeking and Cultural Interpretation. 
                    Each image had up to two QA pairs, ensuring diverse interactions. This approach enabled the model to better capture visual, cultural, and contextual nuances and respond effectively across various linguistic contexts.
                </p>

            </div>

            <div class="subsection">
                <h3 class="text">Curating Existing Multilingual Instructions</h3>
                <p class="text">
                    To further enrich PangeaIns, we conducted an extensive survey of the available multilingual multimodal literature and datasets, including those hosted on HuggingFace. As a result, we incorporated several high-quality, open-source datasets into our PangeaIns mixture. These include Chinese ALLaVA-4V <d-cite key="chen2024allava"></d-cite>, Viet Document and OCR QA <d-cite key="doan2024vintern"></d-cite>, Llava Chinese <d-cite key="ChineseLLaVA"></d-cite>, Llava Medical Chinese Instruction <d-cite key="ChineseLLaVA_Med"></d-cite>, LLaVA-Japanese-Instruct <d-cite key="LLaVA_JP_Instruct_108K"></d-cite>, MTVQA <d-cite key="tang2024mtvqa"></d-cite>, Japanese STAIR Captions <d-cite key="yoshikawa2017stair"></d-cite>, Russian GQA <d-cite key="deepvk2024gqa_ru"></d-cite>, French Doc-VQA <d-cite key="SoSoDocvqa"></d-cite>, and French Table-VQA <d-cite key="AgDeTQA"></d-cite>. Each of these datasets brings unique linguistic and cultural perspectives to the mix, covering a wide range of languages and task types. 
                </p>
            </div>
        </div>

        <div id='model' class="model-block">
            <h1 class="text">Pangea-7B</h1>
            <p class="text">
                We train Pangea-7B on PangeaIns, our multilingual multimodal dataset comprising 6 million samples across 39 languages. The model is based on LLaVA-Next architecture <d-cite key="liu2024llavanext"></d-cite> with Qwen2-7B-Instruct <d-cite key="yang2024qwen2"> as the language model backbone. We employ a learning rate of 2e-5, a batch size of 512, coupled with a cosine decay schedule with 0.03 warmup steps. We train the model for 1 epoch.
            </p>
        </div>

        <div id='benchmarking' class="benchmark-block">
            <h1 class="text">PangeaBench: Evaluation of Multilingual Multimodal Models</h1>
            <p class="text">
                To assess the capabilities of Pangea-7B across a variety of languages, cultures, and task types, we have developed PangeaBench, a comprehensive multilingual and multimodal evaluation suite.
                The overview and examples of PangeaBench from each task are shown in <a href="#fig:eval_data">Figure 4</a>.
            </p>
            <div class="subsection">
                <h3 class="text">Multimodal Tasks</h3>
                
                <p class="text">
                    <strong>Multimodal Chat</strong>:
                    The Multimodal Chat task evaluates a model's ability to engage in dynamic conversations using both text and images. 
                    Multilingual LlavaBench <d-cite key="yu-etal-2022-beyond"></d-cite> stands as the only benchmark for assessing multilingual long-form generation in MLLMs, using coarse-grained evaluation criteria focused on helpfulness, relevance, and accuracy. 
                    However, research shows that these criteria may not align well with human judgment <d-cite key="ye2023flask"></d-cite> <d-cite key="kim2023prometheus"></d-cite> <d-cite key="lee2024prometheusvision"></d-cite> <d-cite key="kim2024biggen"></d-cite> <d-cite key="kim2024prometheus"></d-cite>. 
                    To improve assessment, we developed a new benchmark called xChatBench, featuring fine-grained evaluation criteria across diverse scenarios. 
                    It addresses a common issue where English-centric models respond in English regardless of the query language. This behavior is penalized in xChatBench, receiving a score of zero to emphasize the importance of multilingual accuracy and effective communication. 
                    This strict criterion is essential for enhancing user experience in multilingual contexts.
                </p>
                <p class="text">
                    <strong>Captioning</strong>:
                    The XM100 dataset was created to evaluate models in multilingual image captioning, consisting of images paired with captions in 36 languages <d-cite key="thapliyal2022crossmodal"></d-cite>. 
                    To improve the dataset's diversity and streamline the evaluation, images were clustered based on their captions, and 100 representative images were manually selected from these clusters. 
                    This method reduces redundancy and ensures a broader range of images and captions, making XM100 an effective benchmark for assessing multilingual captioning capabilities.
                </p>
                <p class="text">
                    <strong>Multilingual VQA</strong>:
                    This task evaluates a model's ability to answer questions about images in multiple languages. 
                    The xGQA <d-cite key="pfeiffer2022xgqa"></d-cite> and MaXM <d-cite key="changpinyo2022maxm"></d-cite> datasets offer a wide variety of visual question-answering challenges across different languages and scripts, focusing on cross-lingual visual understanding. 
                    These datasets provide a comprehensive benchmark to assess the model's proficiency in handling diverse linguistic and visual contexts.
                </p>
                <p class="text">
                    <strong>Multi-Subject Reasoning</strong>:
                    The xMMMU and M3Exam <d-cite key="zhang2023m3exam"></d-cite> datasets assess a model's reasoning abilities across various academic subjects. 
                    xMMMU is a machine-translated version of MMMU <d-cite key="yue2024mmmu"></d-cite> validation questions, focusing on multimodal reasoning in multiple subjects. 
                    It includes 300 questions translated into six languages using GPT-4. 
                    M3Exam presents complex, real-world educational questions that require both textual and visual understanding. 
                    These datasets provide a comprehensive benchmark for evaluating models' academic and multimodal reasoning skills. 
                    Further details on the translation quality of xMMMU and descriptions of other datasets are available in the evaluation section.
                </p>
            </div>

            <d-figure id="fig:eval_data">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/xmm_eval_example.png" alt="eval data">
                    <figcaption>
                        <strong>Figure 4:</strong> Overview of PangeaBench, which contains 5 multimodal and 3 text tasks covering 14 datasets (including two newly curated xChatBench and xMMMU datasets). The table provides details about the datasets, while the figure shows evaluation examples from five different multimodal eval tasks in our PangeaBench.
                    </figcaption>
                </figure>
            </d-figure>

            <div class="subsection">
                <h3 class="text">Text-Only Multilingual Datasets</h3>
                <p class="text">
                    While multimodal tasks are critical for evaluating the holistic capabilities of models like PangeaBench, text-only multilingual tasks provide an equally essential dimension to assess.
                    We include three tasks QA, Translation, and Reasoning covering five datasets for the text-only evaluations in PangeaBench.
                    Specifically, we include TydiQA <d-cite key="clark2020tydi"></d-cite> to test the model's ability to answer questions across 11 typologically diverse languages. We adopt the FLORES <d-cite key="nllb2024scaling"></d-cite> dataset to assess machine translation performance across multiple languages. 
                    We sample 11 languages (denoted as FLORES-Sub). We use MMMLU <d-cite key="MMMLU"></d-cite>, a human-translated version of MMLU to test the general language understanding of models. 
                    We use XStoryCloze <d-cite key="lin2022fewshotlearningmultilinguallanguage"></d-cite> and MGSM <d-cite key="shi2022mgsm"></d-cite> to test the model's commonsense and mathematical reasoning ability in multilingual contexts respectively. 
                </p>
            </div>

        </div>

        <div id='eval' class="eval-block">
            <h1 class="text">Evaluation</h1>

            
            <p class="text">
                For evaluation, we compare Pangea-7B against several state-of-the-art open source baselines, including English-centric models Llava-1.5-7B <d-cite key="liu2023improvedllava"></d-cite>, Llava-Next-7B <d-cite key="liu2024llavanext"></d-cite>, Phi-3.5-Vision <d-cite key="abdin2024phi3technicalreporthighly"></d-cite>, Cambrian-8B <d-cite key="tong2024cambrian"></d-cite> and multilingual models PaliGemma-3B <d-cite key="beyer2024paligemma"></d-cite>, PALO-7B <d-cite key="PALO"></d-cite>, mBLIP mT0-XL and mBLIP BLOOMZ <d-cite key="geigle_etal_2024_mblip"></d-cite>. 
                We also consider two text-only LLMs baselines Vicuna-1.5-7B <d-cite key="zheng2023judging"></d-cite> and Qwen2-7B-Instruct <d-cite key="yang2024qwen2"></d-cite>, which are the backbones of Llava-Next and our Pangea-7B respectively. 
                We integrate our multimodal tasks in PangeaBench into <a href="https://github.com/EvolvingLMMs-Lab/lmms-eval" target="_blank">lmms-eval</a> <d-cite key="lmms_eval2024"></d-cite>, a multimodal evaluation package that supports many English multimodal benchmarks. 
                We use <a href="https://github.com/EleutherAI/lm-evaluation-harness" target="_blank">lm-evaluation-harness</a> <d-cite key="biderman2024lessons"></d-cite> to evaluate text-only tasks. 
                We follow the original paper for their best models' prompts in different tasks.
                <a href="#fig:teaser">Figure 1</a> shows the aggregate performance of various multimodal LLMs on PangeaBench.
            </p>

            <div id="tab:final_table" style="display: flex; flex-direction: column; align-items: center;" class="figure">
                <div class="table-container">
                    <table class="data-table">
                        <thead>
                            <tr>
                                <th rowspan="3">Models</th>
                                <th colspan="2">AVG (all)</th>
                                <th colspan="4">Multimodal Chat</th>
                                <th colspan="4">Cultural Understanding</th>
                                <th colspan="4">Captioning</th>
                                <th colspan="4">Short VQA</th>
                                <th colspan="4">Multi-subject Reasoning</th>
                            </tr>
                            <tr>
                                <th colspan="2">AVG (all)</th>
                                <th colspan="2">xChatBench</th>
                                <th colspan="2">M-LlavaBench</th>
                                <th colspan="2">CVQA</th>
                                <th colspan="2">MaRVL</th>
                                <th colspan="2">XM100</th>
                                <th colspan="2">xGQA</th>
                                <th colspan="2">MaXM</th>
                                <th colspan="2">xMMMU</th>
                                <th colspan="2">M3Exam</th>
                            </tr>
                            <tr>
                                <th>en</th>
                                <th>mul</th>
                                <th>en</th>
                                <th>mul</th>
                                <th>en</th>
                                <th>mul</th>
                                <th>en</th>
                                <th>mul</th>
                                <th>en</th>
                                <th>mul</th>
                                <th>en</th>
                                <th>mul</th>
                                <th>en</th>
                                <th>mul</th>
                                <th>en</th>
                                <th>mul</th>
                                <th>en</th>
                                <th>mul</th>
                                <th>en</th>
                                <th>mul</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr class="highlight-gray">
                                <td colspan="22"><i>Proprietary Models</i></td>
                            </tr>
                            <tr>
                                <td>Gemini-1.5-Pro</td>
                                <td>67.1</td>
                                <td>62.5</td>
                                <td>67.0</td>
                                <td>54.4</td>
                                <td>103.4</td>
                                <td>106.6</td>
                                <td>75.9</td>
                                <td>75.7</td>
                                <td>76.4</td>
                                <td>72.0</td>
                                <td>27.6</td>
                                <td>19.1</td>
                                <td>54.2</td>
                                <td>48.7</td>
                                <td>56.4</td>
                                <td>63.5</td>
                                <td>65.8</td>
                                <td>57.7</td>
                                <td>77.4</td>
                                <td>64.7</td>
                            </tr>
                            <tr>
                                <td>GPT4o</td>
                                <td>68.6</td>
                                <td>64.6</td>
                                <td>71.0</td>
                                <td>64.4</td>
                                <td>104.6</td>
                                <td>100.4</td>
                                <td>79.1</td>
                                <td>79.4</td>
                                <td>81.4</td>
                                <td>82.1</td>
                                <td>27.7</td>
                                <td>19.1</td>
                                <td>55.8</td>
                                <td>51.0</td>
                                <td>60.7</td>
                                <td>65.4</td>
                                <td>69.1</td>
                                <td>58.3</td>
                                <td>68.0</td>
                                <td>61.0</td>
                            </tr>
                            <tr class="highlight-gray">
                                <td colspan="22"><i>English Models</i></td>
                            </tr>
                            <tr>
                                <td>Llava-1.5-7B</td>
                                <td>45.4</td>
                                <td>28.4</td>
                                <td>28.5</td>
                                <td>11.8</td>
                                <td>66.1</td>
                                <td>40.8</td>
                                <td>48.9</td>
                                <td>36.5</td>
                                <td>56.2</td>
                                <td>53.7</td>
                                <td>28.6</td>
                                <td>1.1</td>
                                <td>62.0</td>
                                <td>30.6</td>
                                <td>49.8</td>
                                <td>20.4</td>
                                <td>36.2</td>
                                <td>31.5</td>
                                <td>32.3</td>
                                <td>29.0</td>
                            </tr>
                            <tr>
                                <td>Llava-Next-7B</td>
                                <td>51.1</td>
                                <td>32.7</td>
                                <td>40.5</td>
                                <td>18.9</td>
                                <td>78.9</td>
                                <td>50.7</td>
                                <td>55.7</td>
                                <td>42.6</td>
                                <td>62.8</td>
                                <td>50.9</td>
                                <td>29.3</td>
                                <td>9.4</td>
                                <td>64.8</td>
                                <td>37.8</td>
                                <td>54.9</td>
                                <td>21.4</td>
                                <td>36.7</td>
                                <td>34.3</td>
                                <td>36.5</td>
                                <td>28.4</td>
                            </tr>
                            <tr>
                                <td>Phi-3.5-Vision</td>
                                <td>54.0</td>
                                <td>35.0</td>
                                <td>38.5</td>
                                <td>13.2</td>
                                <td>70.8</td>
                                <td>58.0</td>
                                <td>56.3</td>
                                <td>42.3</td>
                                <td>72.1</td>
                                <td>56.5</td>
                                <td>30.2</td>
                                <td>5.2</td>
                                <td>64.7</td>
                                <td>38.4</td>
                                <td>55.3</td>
                                <td>25.0</td>
                                <td>42.6</td>
                                <td>38.8</td>
                                <td>55.8</td>
                                <td>37.2</td>
                            </tr>
                            <tr>
                                <td>Cambrian-8B</td>
                                <td>50.9</td>
                                <td>36.4</td>
                                <td>27.5</td>
                                <td>11.3</td>
                                <td>78.4</td>
                                <td>61.8</td>
                                <td>59.7</td>
                                <td>47.5</td>
                                <td>75.4</td>
                                <td>61.8</td>
                                <td>20.6</td>
                                <td>9.9</td>
                                <td>64.6</td>
                                <td>39.8</td>
                                <td>55.3</td>
                                <td>28.7</td>
                                <td>41.8</td>
                                <td>33.2</td>
                                <td>34.7</td>
                                <td>33.4</td>
                            </tr>
                            <tr>
                                <td>LLaVA-OV-7B</td>
                                <td>59.5</td>
                                <td>41.3</td>
                                <td>51.0</td>
                                <td>28.5</td>
                                <td>89.7</td>
                                <td>55.3</td>
                                <td>65.2</td>
                                <td>53.7</td>
                                <td>72.7</td>
                                <td>57.5</td>
                                <td>30.6</td>
                                <td>7.0</td>
                                <td>64.4</td>
                                <td>48.2</td>
                                <td>54.9</td>
                                <td>34.8</td>
                                <td>46.3</td>
                                <td>41.0</td>
                                <td>60.4</td>
                                <td>45.8</td>
                            </tr>
                            <tr>
                                <td>Molmo-7B-D</td>
                                <td>55.4</td>
                                <td>34.1</td>
                                <td>49.5</td>
                                <td>21.1</td>
                                <td>95.9</td>
                                <td>13.8</td>
                                <td>59.4</td>
                                <td>48.3</td>
                                <td>65.3</td>
                                <td>54.9</td>
                                <td>22.1</td>
                                <td>9.1</td>
                                <td>51.5</td>
                                <td>43.0</td>
                                <td>52.9</td>
                                <td>37.5</td>
                                <td>44.5</td>
                                <td>40.4</td>
                                <td>57.1</td>
                                <td>39.1</td>
                            </tr>  
                            <tr>
                                <td>Llama3.2-11B</td>
                                <td>57.2</td>
                                <td>41.9</td>
                                <td>49.0</td>
                                <td>27.8</td>
                                <td>93.9</td>
                                <td>58.2</td>
                                <td>70.2</td>
                                <td>61.4</td>
                                <td>64.5</td>
                                <td>58.1</td>
                                <td>27.6</td>
                                <td>4.5</td>
                                <td>55.6</td>
                                <td>45.4</td>
                                <td>55.3</td>
                                <td>43.9</td>
                                <td>46.5</td>
                                <td>41.4</td>
                                <td>51.8</td>
                                <td>36.6</td>
                            </tr>
                            <tr class="highlight-gray">
                                <td colspan="22"><i>Multilingual Models</i></td>
                            </tr>
                            <tr>
                                <td>PaliGemma-3B</td>
                                <td>37.3</td>
                                <td>25.8</td>
                                <td>6.0</td>
                                <td>3.5</td>
                                <td>32.1</td>
                                <td>31.9</td>
                                <td>52.9</td>
                                <td>42.9</td>
                                <td>56.5</td>
                                <td>52.2</td>
                                <td>18.7</td>
                                <td>0.8</td>
                                <td>59.7</td>
                                <td>30.5</td>
                                <td>47.9</td>
                                <td>19.9</td>
                                <td>26.3</td>
                                <td>25.2</td>
                                <td>36.0</td>
                                <td>25.6</td>
                            </tr>
                            <tr>
                                <td>PALO-7B</td>
                                <td>46.3</td>
                                <td>32.2</td>
                                <td>27.0</td>
                                <td>11.8</td>
                                <td>68.9</td>
                                <td>71.2</td>
                                <td>50.9</td>
                                <td>39.2</td>
                                <td>63.3</td>
                                <td>54.2</td>
                                <td>30.4</td>
                                <td>0.8</td>
                                <td>60.5</td>
                                <td>37.8</td>
                                <td>51.4</td>
                                <td>16.3</td>
                                <td>33.1</td>
                                <td>30.5</td>
                                <td>30.8</td>
                                <td>27.8</td>
                            </tr>
                            <tr>
                                <td>mBLIP mT0-XL</td>
                                <td>35.1</td>
                                <td>29.8</td>
                                <td>2.5</td>
                                <td>0.5</td>
                                <td>32.7</td>
                                <td>28.2</td>
                                <td>40.5</td>
                                <td>37.5</td>
                                <td>67.3</td>
                                <td>66.7</td>
                                <td>31.9</td>
                                <td>3.1</td>
                                <td>44.2</td>
                                <td>39.9</td>
                                <td>44.7</td>
                                <td>36.8</td>
                                <td>29.3</td>
                                <td>30.4</td>
                                <td>22.8</td>
                                <td>25.0</td>
                            </tr>
                            <tr>
                                <td>mBLIP BLOOMZ</td>
                                <td>36.1</td>
                                <td>30.0</td>
                                <td>4.0</td>
                                <td>1.6</td>
                                <td>43.5</td>
                                <td>41.0</td>
                                <td>44.9</td>
                                <td>36.9</td>
                                <td>62.3</td>
                                <td>58.6</td>
                                <td>22.5</td>
                                <td>10.3</td>
                                <td>43.3</td>
                                <td>36.9</td>
                                <td>44.7</td>
                                <td>24.8</td>
                                <td>29.2</td>
                                <td>30.8</td>
                                <td>30.3</td>
                                <td>29.5</td>
                            </tr>
                            <tr class="highlight-gray">
                                <td colspan="22"><i>Pangea</i></td>
                            </tr>
                            <tr>
                                <td>Pangea-7B (Ours)</td>
                                <td>59.9</td>
                                <td>52.7</td>
                                <td>46.0</td>
                                <td>35.6</td>
                                <td>84.2</td>
                                <td>89.5</td>
                                <td>64.4</td>
                                <td>57.2</td>
                                <td>87.0</td>
                                <td>79.0</td>
                                <td>30.4</td>
                                <td>14.2</td>
                                <td>64.7</td>
                                <td>60.2</td>
                                <td>55.3</td>
                                <td>53.2</td>
                                <td>45.7</td>
                                <td>43.7</td>
                                <td>61.4</td>
                                <td>42.1</td>
                            </tr>
                            <tr>
                                <td>Δ over SoTA Open</td>
                                <td>+0.4</td>
                                <td>+10.8</td>
                                <td>-3.5</td>
                                <td>+7.1</td>
                                <td>-11.7</td>
                                <td>+18.3</td>
                                <td>-5.8</td>
                                <td>-4.2</td>
                                <td>+11.6</td>
                                <td>+12.3</td>
                                <td>-0.2</td>
                                <td>+3.9</td>
                                <td>-0.1</td>
                                <td>+12.0</td>
                                <td>0.0</td>
                                <td>+9.3</td>
                                <td>-0.8</td>
                                <td>+2.3</td>
                                <td>+1.0</td>
                                <td>-3.7</td>
                            </tr>
                                                  
                        </tbody>
                    </table>
                </div>
            </div>
            
            <figcaption style="text-align: center; width: 100%;">
                Table 1: Models' multilingual multimodal evaluation results on PangeaBench.
            </figcaption>

            <p class="text">
                <strong>Multilingual Multimodal Results</strong>
                We show the performance of models on the multimodal tasks from PangeaBench in <a href="#tab:final_table">Table 1</a>.
                The results provide clear insights into the strengths and remaining challenges of Pangea-7B in multilingual and multimodal tasks. Key observations from the evaluation include:
                <br>
                1) Superior English and Multilingual Performance: Pangea-7B outperforms existing open-source models across both English and multilingual tasks. Particularly in cultural understanding (CVQA, MaRVL), it has achieved substantial gains, highlighting its effectiveness in both cross-lingual and cross-cultural contexts.
                <br>
                2) Balanced Cross-Language Capabilities: Unlike many models that exhibit a significant drop in performance when moving from English to multilingual tasks, Pangea-7B is relatively consistent. For instance, in Multimodal Chat tasks, the performance gap between English and multilingual remains relatively small, indicating its ability to handle multiple languages effectively.
                <br>
                3) Challenges Compared to Proprietary Models: While Pangea-7B leads in open-source models, some gaps remain when compared to closed-source models like GPT4o. Additionally, though Pangea-7B narrows the gap between English and multilingual performance, there is still room for improvement in fully closing this divide across all tasks.
            </p>

            <p class="text">
                <strong>Multilingual Text-only Results</strong>
                We further evaluate our model in text-only scenarios in <a href="#tab:text_table">Table 2</a>. Interesting findings include:
                <br>
                1) Best Text Performance Among Multimodal LLMs: Pangea-7B demonstrates the strongest performance among all multimodal LLMs in the text-only tasks consistently outperforming baselines like Llava-Next-7B. This highlights that, despite being trained as a multimodal model, Pangea-7B maintains superior text understanding and reasoning capabilities compared to other MLLMs.
                <br>
                2) Maintained Performance from its Text Backbone:  Pangea-7B generally maintains or sees slight drops in performance on most text-only benchmarks compared with its text backbone Qwen2-7B-Instruct. Notably, the model shows a significant improvement in MGSM. This improvement is directly attributable to the inclusion of math-related instructions in PangeaIns, which enhances the model's capability to handle complex multilingual reasoning and mathematical tasks.
            </p>

            <div id="tab:text_table" style="display: flex; flex-direction: column; align-items: center;" class="figure">
                <div class="table-container">
                    <table class="data-table" style="border-collapse: collapse; width: 100%;">
                        <thead>
                            <tr>
                                <th rowspan="2">Models</th>
                                <th colspan="2">AVG (all)</th>
                                <th colspan="2">FLORES-Sub</th>
                                <th colspan="2">TyDiQA</th>
                                <th colspan="2">XStoryCloze</th>
                                <th colspan="2">MGSM</th>
                                <th colspan="2">MMMLU</th>
                            </tr>
                            <tr>
                                <th>en</th>
                                <th>mul</th>
                                <th>x->en</th>
                                <th>en->x</th>
                                <th>en</th>
                                <th>mul</th>
                                <th>en</th>
                                <th>mul</th>
                                <th>en</th>
                                <th>mul</th>
                                <th>en</th>
                                <th>mul</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Vicuna-1.5-7B</td>
                                <td>52.1</td>
                                <td>38.7</td>
                                <td>55.6</td>
                                <td>42.4</td>
                                <td>59.7</td>
                                <td>52.7</td>
                                <td>78.1</td>
                                <td>57.4</td>
                                <td>17.6</td>
                                <td>6.4</td>
                                <td>49.5</td>
                                <td>34.7</td>
                            </tr>
                            <tr>
                                <td>Qwen2-7B-Instruct</td>
                                <td>66.6</td>
                                <td><b>54.5</b></td>
                                <td><b>61.8</b></td>
                                <td><b>46.0</b></td>
                                <td>72.2</td>
                                <td><b>71.2</b></td>
                                <td><b>80.3</b></td>
                                <td><b>61.9</b></td>
                                <td>48.8</td>
                                <td>40.4</td>
                                <td><b>70.1</b></td>
                                <td><b>53.1</b></td>
                            </tr>
                            <tr>
                                <td>Llava-1.5-7B</td>
                                <td>53.1</td>
                                <td>39.0</td>
                                <td>54.7</td>
                                <td>41.5</td>
                                <td>66.8</td>
                                <td>52.8</td>
                                <td>79.1</td>
                                <td>57.6</td>
                                <td>14.8</td>
                                <td>7.6</td>
                                <td>50.2</td>
                                <td>35.7</td>
                            </tr>
                            <tr>
                                <td>Llava-Next-7B</td>
                                <td>54.0</td>
                                <td>38.9</td>
                                <td>54.8</td>
                                <td>41.4</td>
                                <td>68.3</td>
                                <td>52.1</td>
                                <td>79.1</td>
                                <td>57.1</td>
                                <td>15.6</td>
                                <td>7.5</td>
                                <td>52.1</td>
                                <td>36.5</td>
                            </tr>
                            <tr>
                                <td>Phi-3.5-Vision</td>
                                <td>60.7</td>
                                <td>41.7</td>
                                <td>28.5</td>
                                <td>32.5</td>
                                <td><b>75.9</b></td>
                                <td>51.3</td>
                                <td>77.9</td>
                                <td>54.8</td>
                                <td>59.2</td>
                                <td>33.1</td>
                                <td>62.0</td>
                                <td>36.7</td>
                            </tr>
                            <tr>
                                <td>PALO-7B</td>
                                <td>52.0</td>
                                <td>37.5</td>
                                <td>52.9</td>
                                <td>40.4</td>
                                <td>69.4</td>
                                <td>50.8</td>
                                <td>77.4</td>
                                <td>57.2</td>
                                <td>13.6</td>
                                <td>5.8</td>
                                <td>46.7</td>
                                <td>33.4</td>
                            </tr>
                            <tr>
                                <td>PANGEA-7B (Ours)</td>
                                <td><b>72.8</b></td>
                                <td>54.3</td>
                                <td>60.7</td>
                                <td>44.9</td>
                                <td>73.7</td>
                                <td>66.0</td>
                                <td>79.1</td>
                                <td>61.2</td>
                                <td><b>82.0</b></td>
                                <td><b>47.4</b></td>
                                <td>68.4</td>
                                <td>52.2</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
            <figcaption style="text-align: center; width: 100%;">
                Table 2: Models' multilingual text-only evaluation results on PangeaBench.
            </figcaption>
        

        </div>

        <div id='discussion' class="discussion-block">
            <h1 class="text">Discussion</h1>

            <d-figure id="fig:scaling_effect">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/scaling_effect.png" alt="scaling effect">
                    <figcaption>
                        <strong>Figure 5:</strong> Scaling effect of training samples on English and multilingual scores across datasets.
                    </figcaption>
                </figure>
            </d-figure>

            <p class="text">
                <strong>Scaling Effect of Number of Instructions:</strong>
                Understanding how the quantity of instructions affects model performance is crucial for optimizing training strategies and resource allocation. 
                <a href="#fig:scaling_effect">Figure 5</a> reveals a clear scaling effect related to the number of instructions used during training. 
                Performance improvements were consistent as we increased the number of multilingual instructions in PangeaIns, for both English and multilingual performance. 
                This demonstrates the necessity of scaling multilingual multimodal instruction tuning. 
            </p>

            <d-figure id="fig:english_ratio" style="display: flex; justify-content: center;">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/english_vs_multilingual.png" alt="english_vs_multilingual" style="width: 60%" class="center">
                    <figcaption>
                        <strong>Figure 6:</strong> Impact of English training data proportion on English vs. multilingual performance.
                    </figcaption>
                </figure>
            </d-figure>

            <p class="text">
                <strong>Role of English Data:</strong>
                In multilingual scenarios, English data plays a pivotal role in cross-lingual transfer. 
                To investigate this, we sampled 500K examples from the translated data described in <a href="#machine_translation">Machine Translated Instructions</a>, ensuring a consistent data distribution. 
                We varied the ratio of English data while keeping the total number of training samples fixed at 500K. 
                For the 17 multilingual languages in the translated subset, we evenly distributed the number of samples across languages. 
                As shown in <a href="#fig:english_ratio">Figure 6</a>, English performance generally improves as the percentage of English data increases. 
                More surprisingly, using no English data (full multilingual data) results in relatively lower multilingual performance. 
                As we introduce more English data, multilingual performance improves, peaking at 38.7% with 40% English. 
                However, performance drops sharply when English data reaches 100%. 
                This suggests that English data aids cross-lingual transfer, however, over-reliance on it harms multilingual performance. 
            </p>
        </div>

        <d-figure id="fig:language_portion_downstream_performance">
            <figure>
                <img data-zoomable="" draggable="false" src="static/img/train_eval_perf_percentage.png" alt="train_eval_perf_percentage">
                <figcaption>
                    <strong>Figure 7:</strong> The relationship between training sample size (relative to English) and performance (relative to English) of different languages across four datasets.
                </figcaption>
            </figure>
        </d-figure>

        <p class="text">
            <strong>How does the proportion of training samples in a language affect downstream performance?</strong>
            An interesting question to ask is whether the downstream task performance is correlated with the number of training samples. 
            Our analysis in <a href="#fig:language_portion_downstream_performance">Figure 7</a> revealed a nuanced relationship between training sample proportion and downstream performance. 
            While there is a general positive correlation, the impact varies significantly across languages and tasks. 
            For widely spoken languages with rich linguistic resources, we observed a near-linear relationship. 
            However, for low-resource languages, even a small increase in proportion yielded disproportionately large performance gains. 
            Interestingly, we also noted instances of positive transfer between typologically similar languages. 
            These findings suggest that strategic allocation of training samples, considering both language prevalence and linguistic similarities, can optimize overall model performance.
        </p>

        <d-figure id="fig:ocr_accuracy" style="display: flex; justify-content: center;">
            <figure>
                <img data-zoomable="" draggable="false" src="static/img/ocr_accuracy.png" alt="ocr_accuracy" style="width: 60%" class="center">
                <figcaption>
                    <strong>Figure 8:</strong> A preliminary exploration of multilingual OCR.
                </figcaption>
            </figure>
        </d-figure>

        <p class="text">
            <strong>Preliminary Explorations of Multilingual OCR:</strong>
            Multilingual OCR emerged as a particularly challenging aspect of Pangea's functionality. 
            We made efforts to enhance its multilingual OCR capabilities. 
            Specifically, we constructed a dataset of 500K multilingual OCR instructions spanning 10 languages, with 50K examples per language, sourced from web user interfaces. 
            Webpages naturally serve as image-rich environments containing abundant text, and by capturing screenshots of websites from various countries in different languages, we were able to gather a substantial number of OCR images. 
            We employed the same model architecture as Pangea but trained it exclusively on these OCR images, reserving a portion of the data as a test set. As shown in <a href="#fig:ocr_accuracy">Figure 8</a>, the results indicate that improving multilingual OCR performance is feasible with an increase in training samples. 
            However, the OCR accuracy for non-Latin scripts (e.g., Chinese, Japanese, and Korean) remains lower than for Latin-based languages. 
            Looking ahead, we aim to further expand the multilingual OCR training dataset to include more languages and integrate this data into PangeaIns.
        </p>

    </div>

        <div id="conclusion" style="position: relative; margin-top: 40px; margin-bottom: 0px;">
            <h2 class="text" style="margin-top:0px; margin-bottom:10px">Conclusion</h2>
            <p class="text">
                We introduced Pangea, a novel multilingual multimodal large language model designed to bridge linguistic and cultural gaps in visual understanding tasks. 
                By leveraging PangeaIns, our newly curated 6M multilingual multimodal instruction data samples, we demonstrated significant improvements in cross-lingual and cross-cultural understanding across 39 typologically diverse languages. 
                Our comprehensive evaluation using PangeaBench revealed Pangea's superior performance compared to existing open-source models, particularly in tasks requiring nuanced cultural interpretation. 
                We also highlight ongoing challenges in areas such as low-resource language support and multilingual OCR. 
                We fully open-source Pangea, PangeaIns, and PangeaBench to facilitate future research to build open and inclusive MLLMs. 
            </p>
        </div>

        <div id="acknowledgement" style="position: relative; margin-top: 40px; margin-bottom: 0px;">
            <h2 class="text" style="margin-top:0px; margin-bottom:10px">Acknowledgement</h2>
            <p class="text">
                This work was supported in part by the Carnegie Bosch Institute Fellowship and a grant from DSTA Singapore.  
                The training is supported by the CMU FLAME Center. 
                The authors would like to thank CMU NeuLab colleagues for their constructive comments. 
                The authors would like to thank Google Gemini credits for data construction and evaluation.
                The authors would like to thank Cambrian team for their <a href="https://cambrian-mllm.github.io/" target="_blank">project webpage template</a>. 
            </p>
        </div>

        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @article{tong2024cambrian,<br>
                &nbsp;&nbsp;title={{Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs}},<br>
                &nbsp;&nbsp;author={Tong, Shengbang and Brown, Ellis and Wu, Penghao and Woo, Sanghyun and Middepogu, Manoj and Akula, Sai Charitha and Yang, Jihan and Yang, Shusheng, and Iyer, Adithya and Pan, Xichen and Wang, Austin and Fergus, Rob and LeCun, Yann and Xie, Saining},<br>
                &nbsp;&nbsp;journal={arXiv preprint arXiv:2406.16860},<br>
                &nbsp;&nbsp;year={2024}<br>
                }
            </p>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>   
        <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
        <d-bibliography src="bibliography.bib"></d-bibliography>
        <script src="./static/js/nav-bar.js"></script>
    </body>
</html>
